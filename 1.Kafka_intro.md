

# Concepts 


Before understanding the kafka , it is important to understand - what is an event ?

## Event 

- Anything that has happened 
    eg - a click on website, a temp raise in thermostat, a sms , a birthday

- An event is any type of action, incident or change that is identified by software or applications

- An event is combination of notification(Element of when-ness + Trigger some other activity: ) + State(It can provide insight into its current lifecycle or what has happened to it as it progresses through a system). State is presented in the structure format such as JSON

### Kafka and Events – Key/Value Pairs

Kafka processes events, which are messages or records of something that has happened, like a user placing an order or a sensor sending temperature data. Each of these events is represented as a key/value pair.

#### Key

- This is an identifier for some entity (e.g., a user ID, an order ID, or a device ID). It helps Kafka determine which partition of the log the event will be stored in, which is important for parallel processing and efficient data retrieval.

- The key is typically used to identify the entity related to the event (e.g., a user ID, device ID, or order ID). However, the key is not always unique to the event itself. It’s more like a way to organize and manage events related to the same entity. For example, multiple events related to the same user would have the same key (the user ID).

#### Value

- This is the actual content or data of the event (e.g., the details of an order, temperature readings from a sensor, or user information). 

- The value is the main data of the event. For example, if you're sending an event about a purchase, the value could contain details like the item name, quantity, and price. This data is serialized and stored in Kafka

#### What is a Log in Kafka?
Kafka is built around the concept of a distributed commit log. Think of a log like an ordered list of events, where new events are appended at the end. This log is split into partitions, which allows Kafka to scale by distributing the data across multiple servers


#### Serialization and Deserialization
Events in Kafka (the keys and values) are just sequences of bytes internally. However, when working with Kafka in application, these bytes need to be converted into more meaningful data types, like integers, strings, or objects, depending on the programming language you're using.

- Serialization: This is the process of converting structured data (like an object or record in application) into a byte sequence that Kafka can store.
- Deserialization: This is the reverse process, where Kafka turns the byte sequence back into a structured object that application can understand.

#### Why Keys Are Important
While the key might not seem important at first, it plays a critical role in how Kafka handles parallelization (processing multiple events at the same time) and data locality (keeping related data close together).

- Parallelization: Kafka can divide events into partitions, which are stored across different servers. By assigning events with the same key to the same partition, Kafka ensures that events related to the same entity are processed in order, even if many partitions are being processed at the same time.

- Data Locality: Events with the same key will always go to the same partition. This is important for ensuring that related data (e.g., all events related to a specific user or device) are grouped together, making it more efficient to process.

Example
Let’s say there is a temperature sensor that sends readings to Kafka:

Key: The sensor’s ID (e.g., "sensor_123").
Value: The actual temperature reading (e.g., "25°C").
Kafka stores these events in a log, partitioned by sensor ID (the key), and the temperature readings (the value) are serialized into a format like JSON or Avro.

Summary
Kafka stores and processes events as key/value pairs. The key helps Kafka manage parallel processing and keep related data together, while the value is the actual data you care about (like a message or a sensor reading). To make it efficient and scalable, Kafka uses serialization (converting structured data into bytes) and deserialization (turning bytes back into data) for its internal storage and processing.

